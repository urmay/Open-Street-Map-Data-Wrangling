{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Street Map Data Wrangling with SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Summary <a name=\"top\"></a>\n",
    "\n",
    "\n",
    "### Objective: \n",
    "Thoroughly audit and clean the OSM dataset for you selected city, converting it from XML to CSV format. Then importing the cleaned .csv data file into a Sqlite database and analyze insights within the data.\n",
    "\n",
    "#### Map area:\n",
    "+ Location: Ahmedabad, Gujarat, India\n",
    "- [Mapzen URl for Ahmedabad, Gujarat, India](https://mapzen.com/data/metro-extracts/metro/ahmedabad_india)\n",
    "\n",
    "#### Reason for the choice of area: \n",
    "I have chosen Ahmedabad city's area as it is my home town.So i want to explore more about the my city.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents \n",
    "----\n",
    "[](#top)\n",
    "0. [Data Audit](#audit)\n",
    "1. [Problems Encountered](#problems)\n",
    "2. [Data Overview with SQL](#data_overview)\n",
    "3. [Exploring Data further with SQL](#exploration)\n",
    "4. [Conclusion](#conclusion)\n",
    "5. [References](#reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a name=\"audit\"></a> **1. Data Audit**</h2>\n",
    "> #### What is Data Auditing?\n",
    "    Data auditing is the process of programatically checking the data using some validation rules that are written.This involves profiling the data and assessing the impact of poor quality data on the data performance.\n",
    "\n",
    "     Below is the code I have used for auditing the OSM XML file of Ahmedabad city just to have idea about the data.(code can be found in check.py file).Besides this full auditing file can be found in audit.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Area Name    Latitude   Longitude\n",
      "0           Ahmedabad  23.0216238  72.5797068\n",
      "1              Naroda  23.0861141  72.6584406\n",
      "2           Vastrapur  23.0081745  72.5193869\n",
      "3           Maninagar  22.9980691  72.6118069\n",
      "4  Sabarmati Junction  23.0778134  72.5885153\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "datafile = \"india_ahmedabad.osm\" # Name of the data set used while auditing\n",
    "OSMFILE = datafile\n",
    "\n",
    "tree = ET.parse(datafile)\n",
    "root = tree.getroot()\n",
    "allnodes=root.findall('node')\n",
    "\n",
    "areaname = [] # set as a null list in case proper tag not found\n",
    "dfTupleList = []\n",
    " \n",
    "for node in allnodes:\n",
    "    latitude = node.get('lat')\n",
    "    longitude = node.get('lon')\n",
    "    \n",
    "    for tag in node.findall('tag'):\n",
    "        if tag.attrib['k'] == 'name':\n",
    "            # add code here to get the cityname\n",
    "            areaname.append(tag.attrib['v'])\n",
    "            #Printing the list\n",
    "            dfTupleList.append((tag.attrib['v'],latitude,longitude))\n",
    "\n",
    "df = pd.DataFrame(dfTupleList)\n",
    "df.columns = ['Area Name','Latitude','Longitude']\n",
    "print df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I have parsed through the Ahmedabad City dataset with ElementTree and counted the number of unique element types to get an overall understanding of the data by using count_tags function.\n",
    "\n",
    "* with the use of parsing code which is written in \"mapparser.py\" file we got the following output which shows the no of unique tags\n",
    "\n",
    "    **'bounds': 1,\n",
    "     'member': 2278,\n",
    "     'nd': 657695,\n",
    "     'node': 565470,\n",
    "     'osm': 1,\n",
    "     'relation': 510,\n",
    "     'tag': 102739,\n",
    "     'way': 84533**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the functions __key_type_category__ & __processing_map__. We check the value of \"k\" for each \"tag\" and see if they can be suitable keys in SQL. We also check whether there are any other potential problems.\n",
    "* As we saw in the earlier, we would like to change the data model and expand the \"addr:street\" type of keys to a dictionary like this:\n",
    "{\"address\": {\"street\": \"Some value\"}}\n",
    "So, we have to see if we have such tags, and also if we have any tags with problematic characters.\n",
    "\n",
    "* For the function 'key_type', we count for each of three tag categories in a dictionary:\n",
    "\n",
    "    1.  __\"LOWER\"__, for tags that contain only lowercase letters and are valid,\n",
    "    2.  __\"LOWER_COLON\"__, for otherwise valid tags with a colon in their names,\n",
    "    3.  __\"PROBLEMCHARS\"__, for tags with problematic characters, and\n",
    "\n",
    "    ** Below is the count for each four categories: **\n",
    "\n",
    "    **'LOWER': 100647, 'LOWER_COLON': 2051, 'PROBLEMCHARS': 7, 'other': 34**\n",
    "    \n",
    "    \n",
    "* During analysis we found that there are **387** unique users have contributed for ahmedabad city map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a name=\"problems\"></a> **2. Problem encountered**</h2>\n",
    "\n",
    ">It is found that many streets had abbreviations,many streets names were written in different languages,many had speeling mistakes.It is also found that field pincode.zipdoce have invalid number written or have less digits may be because of manual entry.As ahmedabad city have pin code starting from 38 remaining other pincodes were considered as invalid. In this section, I have audited only Street abbreviation and pincode issue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name=\"street\"></a> **Street address abbreviation **</h3>\n",
    "* The main problem we encountered in this dataset are the street name abbreviation inconsistency. Many streets had abbreviations, many had spelling mistakes, and some were written all together in a different language itself. \n",
    "\n",
    "* I have a regex matching the last element in the string, where usually the street type is provided. Also there is a list of mapping which that need not to be cleaned and a expectance list, that will be used for matching / validating the data.Below is the old name corrected with the better name. (Using audit.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#expected names in the dataset\n",
    "expected = [\"Ahmedabad\", \"Road\",\"Feet\", \"NR\", \"Avenue\", \"SBK\", \"Gandhi\", \"Bridge\", \"Society\",\n",
    "                \"Gujarat\",\"Airport\", \"Satellite\", \"Square\",\" Bus Rapid Transit System(BRTS)\",\"Bapunagar\",\n",
    "               \"Circle\",\"Crossroad\",\"Area\"]\n",
    "#old name:better name  \n",
    "\n",
    "mapping = {\"ahmedabad\": \"Ahmedabad\",\"Ahmadabad\": \"Ahmedabad\", \"Ahamadabad\": \"Ahmedabad\",\n",
    "            \"Nr.\": \"NR\",\n",
    "            \"Ave.\": \"Avenue\",\n",
    "            \"sbk\": \"SBK\",\n",
    "            \"gandhi\": \"Gandhi\",\n",
    "            \"bridge\": \"Bridge\",\n",
    "            \"Ft.\": \"Feet\",\n",
    "            \"ft\": \"Feet\",\n",
    "            \"road\": \"Road\",\"Rd\": \"Road\",\"Rd.\": \"Road\",\"rasta\": \"Road\",\"Roads\": \"Road\",\"Marg\": \"Road\",\"orad\": \"Road\",\"way\": \"Road\",\"रोड\" : \"Road\",# 'रोड' = Road in Hindi Language\n",
    "            \"society\": \"Society\",\"soc.\": \"Society\",\"Socity\": \"Society\",\"Sosciety\": \"Society\",\n",
    "            \"Gujarat.\": \"Gujarat\",\n",
    "            \"एरपोर्ट\" : \"Airport\",# \"एरपोर्ट\" = Aiport in Hindi Language\n",
    "            \"Settellte\" : \"Satellite\",\n",
    "            \"SQUAR\": \"Square\",\n",
    "            \"BRTS\" : \" Bus Rapid Transit System(BRTS)\",\n",
    "            \"Bapuangar\" :\"Bapunagar\",\n",
    "            \"Circle,\" : \"Circle\",\n",
    "            \"chokdi\":\"Crossroad\",\"Crossroads\": \"Crossroad\",\"area\": \"Area\",\"char rasta\":\"Crossroad\"\n",
    "                }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Converting xml to csv : we have converted xml data into following individual csv and made database.Below is the list. \n",
    "\n",
    "        1.nodes.csv\n",
    "        2.nodes_tags.csv\n",
    "        3.ways.csv\n",
    "        4.ways_nodes.csv\n",
    "        5.ways_tags.csv\n",
    "        5.ahmedabad1.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a name=\"data_overview\"></a> **3. Data Overview with SQL**</h2>\n",
    "> In this section we will be exploring the ahmedabad database.We have used SQL for the exploration purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Number of nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def number_of_nodes():\n",
    "\tresult = cur.execute('SELECT COUNT(*) FROM nodes')\n",
    "\treturn result.fetchone()[0]\n",
    "\n",
    "print \"Number of nodes: \" , number_of_nodes()\n",
    "\n",
    "\n",
    "#OUTPUT\n",
    "Number of nodes:  565470"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Number of ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def number_of_ways():\n",
    "\tresult = cur.execute('SELECT COUNT(*) FROM ways')\n",
    "\treturn result.fetchone()[0]\n",
    "\n",
    "print \"Number of ways: \" , number_of_ways()\n",
    "\n",
    "# OUTPUT\n",
    "Number of ways:  84533"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Number of Unique Users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def number_of_unique_users():\n",
    "\tresult = cur.execute('SELECT COUNT(DISTINCT(e.uid)) \\\n",
    "            FROM (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways) e')\n",
    "\treturn result.fetchone()[0]\n",
    "\n",
    "print \"Number of unique users: \" , number_of_unique_users()\n",
    "\n",
    "#OUTPUT\n",
    "Number of unique users:  382"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Top contributing Users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_contributing_users():\n",
    "\tusers = []\n",
    "\tfor row in cur.execute('SELECT e.user, COUNT(*) as num \\\n",
    "            FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e \\\n",
    "            GROUP BY e.user \\\n",
    "            ORDER BY num DESC \\\n",
    "            LIMIT 10'):\n",
    "\t\tusers.append(row)\n",
    "\treturn users\n",
    "\n",
    "print \"Top contributing users: \" , top_contributing_users()\n",
    "\n",
    "#OUTPUT\n",
    "\n",
    "Top contributing users:  [(u'uday01', 177284), \n",
    "                          (u'sramesh', 136607), \n",
    "                          (u'chaitanya110', 122213),\n",
    "                          (u'shashi2', 49502),\n",
    "                          (u'shravan91', 22900),\n",
    "                          (u'vkvora', 21952), \n",
    "                          (u'shiva05', 19669), \n",
    "                          (u'Rajsamand Local Guide', 13914),\n",
    "                          (u'bhanu3', 12512),\n",
    "                          (u'Oberaffe', 7056)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Number of users contributing only once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def number_of_users_contributing_once():\n",
    "\tresult = cur.execute('SELECT COUNT(*) \\\n",
    "            FROM \\\n",
    "                (SELECT e.user, COUNT(*) as num \\\n",
    "                 FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e \\\n",
    "                 GROUP BY e.user \\\n",
    "                 HAVING num=1) u')\n",
    "\treturn result.fetchone()[0]\n",
    "\n",
    "print \"Number of users contributing once: \" , number_of_users_contributing_once()\n",
    "\n",
    "#OUTPUT\n",
    "Number of users contributing once:  92"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a name=\"exploration\"></a> **4. Exploring Data further with SQL**</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Finding Common Ammenities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def common_ammenities():\n",
    "    amenities=[]\n",
    "    \n",
    "    for row in cur.execute('SELECT value, COUNT(*) as num \\\n",
    "            FROM nodes_tags \\\n",
    "            WHERE key=\"amenity\" \\\n",
    "            GROUP BY value \\\n",
    "            ORDER BY num DESC \\\n",
    "            LIMIT 10'):\n",
    "        amenities.append(row)\n",
    "    return amenities\n",
    "\n",
    "print \"Common ammenities: \" , common_ammenities()\n",
    "\n",
    "#OUTPUT\n",
    "\n",
    "Common ammenities:  [(u'place_of_worship', 71), (u'restaurant', 50), (u'hospital', 33), (u'bank', 32),\n",
    "                     (u'school', 27), (u'atm', 24), (u'fuel', 23), (u'fast_food', 22), (u'cafe', 16), (u'library', 14)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Finding Top Religions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_religion():\n",
    "    religion_list =[]\n",
    "    \n",
    "    for row in cur.execute('SELECT value, COUNT(*) as num \\\n",
    "            FROM nodes_tags \\\n",
    "                JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value=\"place_of_worship\") i \\\n",
    "                ON nodes_tags.id=i.id \\\n",
    "            WHERE nodes_tags.key=\"religion\" \\\n",
    "            GROUP BY nodes_tags.value \\\n",
    "            ORDER BY num DESC \\\n",
    "            LIMIT 5'):\n",
    "        religion_list.append(row)\n",
    "\t\n",
    "    return religion_list\n",
    "\n",
    "print \"Top Religion: \" , top_religion()\n",
    "\n",
    "#OUTPUT \n",
    "\n",
    "Top Religion:  [(u'hindu', 35), (u'jain', 5), (u'muslim', 4), (u'christian', 2), (u'nonsectarian', 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Popular Cuisines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def popular_cuisines():\n",
    "    cuisines_list =[]\n",
    "    \n",
    "    for row in cur.execute('SELECT value, COUNT(*) as count \\\n",
    "            FROM nodes_tags \\\n",
    "            JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value=\"restaurant\") i \\\n",
    "            ON nodes_tags.id=i.id \\\n",
    "            WHERE nodes_tags.key=\"cuisine\" \\\n",
    "            GROUP BY nodes_tags.value \\\n",
    "            ORDER BY count DESC\\\n",
    "            LIMIT 10'):\n",
    "        cuisines_list.append(row)\n",
    "        \n",
    "    return cuisines_list\n",
    "print \"Popular Cuisines: \" , popular_cuisines()\n",
    "\n",
    "#OUTPUT\n",
    "Popular Cuisines:  [(u'regional', 7), (u'indian', 3), (u'vegetarian', 3), (u'pizza', 2),\n",
    "                    (u'Punjabi,_SouthIndia,_Gujarati Thali.', 1), (u'burger', 1),\n",
    "                    (u'burger;sandwich;regional;ice_cream;grill;cake;coffee_shop;pasta;noodles;pancake;pizza;chicken;fish_and_chips;curry;indian;vegan;fish;breakfast;savory_pancakes;tea;seafood;sausage;local;barbecue;vegetarian', 1),\n",
    "                    (u'international', 1), (u'italian', 1), (u'sandwich', 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Top List of Postcodes/Pincodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_postcodes():\n",
    "    top_postcode_list=[]\n",
    "    \n",
    "    \n",
    "    for row in cur.execute('SELECT value, COUNT(*) as count\\\n",
    "                           FROM nodes_tags \\\n",
    "                           WHERE key = \"postcode\"\\\n",
    "                           GROUP BY nodes_tags.value \\\n",
    "                           ORDER BY count DESC\\\n",
    "                           LIMIT 10'):\n",
    "        top_postcode_list.append(row)\n",
    "    return top_postcode_list\n",
    "print \"Top postcodes:\",top_postcodes()\n",
    "#OUTPUT\n",
    "Top postcodes: [(u'380009', 14), (u'380015', 13), (u'380054', 11), (u'380001', 10), \n",
    "                (u'380006', 9), (u'380008', 9), (u'382480', 7), (u'380007', 4), (u'380013', 4), (u'382481', 4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Top Types of Buildings found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_type_of_buildings():\n",
    "    building_list=[]\n",
    "    \n",
    "    \n",
    "    for row in cur.execute('SELECT value, COUNT(*) as count\\\n",
    "                           FROM ways_tags \\\n",
    "                           WHERE key = \"building\"\\\n",
    "                           GROUP BY ways_tags.value \\\n",
    "                           ORDER BY count DESC\\\n",
    "                           LIMIT 10'):\n",
    "        building_list.append(row)\n",
    "    return building_list\n",
    "print \"Top Types of Buildings:\",top_type_of_buildings()  \n",
    "\n",
    "#OUTPUT\n",
    "Top Types of Buildings: [(u'yes', 70471), (u'house', 776), (u'apartments', 374),\n",
    "                         (u'commercial', 139),(u'residential', 64), (u'university', 29),\n",
    "                         (u'industrial', 23), (u'school', 20), (u'college', 8), (u'dormitory', 6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<div align=\"center\">Back to top</div>](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a name=\"conclusion\"></a> **5. Conclusion**</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Benifits of the Map Data:_**\n",
    "\n",
    "> Using this data analysis of various region an be done easily.It can very useful for the market researcher as it gives lots information about the no of restaturants and cafes in the particular area.Using this one can analyze in which area no of cafes are less which directly affect the the business.Same is the case with amenities (like bank,hospitals) government can also do study using this data to improve growth of the particular area\n",
    "\n",
    "\n",
    "> **_Ideas to improve data quality of OSM:_**\n",
    "\n",
    "> When we audited the data, it was clear that although there were minor errors which might be cause by human input or might be just some random error but the dataset is fairly well-cleaned. Considering there are hundreds of contributors for this map, there is a great numbers of human errors in this project. I would recommend a structured input form,  so that every user who wants to contribute for the Map Community, can input the same data via the form so that it is entered in a Structural way.This will reduce these kind of errors . These structures can also have certain field formats which will be validated as the user enters the data.Also, we can create a robust script to clean the data on a weekly or regularly basis. \n",
    "\n",
    "> We can also develope the script which cleans the data on regular basis like after every six months.It may require the special team to authenticate the cahnges.To clean whole data every 6 months required use of Big dataframewrok like apache spark for implementation.\n",
    "\n",
    "> For adding cusines or information related to food places we can also intigrate with the data from food app like zomato(India's top rated food App) for more accuracy.As zomato is independent app it is require to officially buy information about the food places.Integration of the only food related area/restaurant with general map may be difficult but use of indiavidual layers of only food iteam places on original map data can help.Updation/Insertion of Id in each file may cause the problem that we need look in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a name=\"reference\"></a> **6. References**</h2>\n",
    "\n",
    "- Udacity's \"Data Wrangling with SQL\" subcourse under Data Analyst Course\n",
    "\n",
    "- [Name finder:Abbreviations Guide , OpenstreetMap Wiki](http://wiki.openstreetmap.org/wiki/Name_finder:Abbreviations) \n",
    "\n",
    "- [Sqlite basic commands](https://www.sitepoint.com/getting-started-sqlite3-basic-commands/)\n",
    "\n",
    "- [OpenStreetMap Data Wrangling with SQL by Pratyush Kumar](https://github.com/pratyush19/Udacity-Data-Analyst-Nanodegree/tree/master/P3-OpenStreetMap-Wrangling-with-SQL)\n",
    "\n",
    "- [Wiki for Openstreetmap](http://wiki.openstreetmap.org/)\n",
    "\n",
    "- [Data auditing wikipedia](https://en.wikipedia.org/wiki/Data_auditing)\n",
    "\n",
    "- [What is Data Wrangling?](http://www.datawatch.com/what-is-data-wrangling/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files included in the Folder\n",
    "\n",
    "    1.india_ahmedabad.osm : original osm file \n",
    "    2.india_ahmedabad_sample.osm : sample data of the OSM file\n",
    "    3.audit.py : audit street, city and update their names\n",
    "    4.data.py : build CSV files from OSM and also parse, clean and shape data\n",
    "    5.database.py : create database of the CSV files\n",
    "    6.mapparser.py : find unique tags in the data\n",
    "    7.query.py : different queries about the database using SQL\n",
    "    8.sample.py : extract sample data from the OSM file\n",
    "    9.Data_Wrangling_With_SQL.ipynb : Jupyter notebook containing the code as well as documentation for easy implementation \n",
    "    of the project\n",
    "    10.Data_Wrangling_with_SQL.html : html of this notebook\n",
    "    11.Data_Wrangling_with_SQL.md : Markdown of this notebook\n",
    "    12.check.py : contains overview function script\n",
    "    13.schema.py : Contains schema file took from udacity course file for conversion.\n",
    "    14.ahmedabad1.db :contains the database of ahmedabad.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
